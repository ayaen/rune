//  Copyright 2023 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import database as db
use keytab
use lexer
use sym

enum TokenType {
  Keyword
  Ident
  Integer
  Float
  Bool
  String
  WeakString  // Only used in parsing PEG rules
  Eof
  RandUint
  IntType
  UintType  // If this is not the last anymore, fix code that assumes this.
}

class Token(self, lexer: Lexer, type: TokenType, location: db.Location, keyword: Keyword? = null(Keyword),
    value: db.Value? = null(db.Value)) {
  self.type = type
  self.location = location
  // We should use a tagged union here.
  if !isnull(keyword) {
    keyword.appendToken(self)
  }
  self.value = value
  lexer.appendToken(self)

  // Pass in the text from the lexer.
  func dump(self) {
    self.location.dump()
  }

  // Return the token's text.
  func getName(self) {
    if self.type == TokenType.Eof {
      return "EOF"
    }
    location = self.location
    filepath = self.lexer.filepath!
    return filepath.text[location.pos : location.pos + location.len]
  }

  func isValue(self, value: string | bool | Int | Uint | db.Bigint | Float | Sym) -> bool {
    if isnull(self.value) {
      return false
    }
    v = db.Value(value)
    return v == self.value!
  }

  func isKeyword(self, name: string) -> bool {
    if isnull(self.keyword) {
      return false
    }
    return Sym.new(name) == self.keyword.sym
  }

  // Return true if this is the eof token.
  func eof(self) {
    return self.type == TokenType.Eof
  }

  // Create a new token from a value.  This works for values like 123u8, and
  // also for identifiers which are parsed as Sym values, and keywords.
  func newValueToken(lexer: Lexer,
      value: string | bool | Int | Uint | db.Bigint | Float | Sym | Keyword,
      location: db.Location) -> Token {
    typeswitch value {
      string => return Token(lexer, TokenType.String, location, value = db.Value(value))
      bool => return Token(lexer, TokenType.Bool, location, value = db.Value(value))
      Int, Uint, db.Bigint => return Token(lexer, TokenType.Integer, location,
          value = db.Value(value))
      Float => return Token(lexer, TokenType.Float, location, value = db.Value(value))
      Sym  => return Token(lexer, TokenType.Ident, location, value = db.Value(value))
      Keyword  => return Token(lexer, TokenType.Keyword, location, keyword = value)
    }
  }
}

relation DoublyLinked Keyword Token cascade
relation ArrayList Lexer Token cascade

unittest {
  // Unnamed unit tests declare stuff common to all unit tests in the file.
  import database.filepath as fp
}

unittest dumpTest {
  filepath = fp.Filepath.new("test_filepath", null(fp.Filepath), false)
  filepath.text = "first line\nsecond line"
  keytab = Keytab()
  lexer = Lexer(filepath, keytab, false)
  token1 = Token(lexer, TokenType.Ident, db.Location(filepath, 0u32, 5u32, 1u32))
  token2 = Token(lexer, TokenType.Ident, db.Location(filepath, 6u32, 4u32, 1u32))
  newline = Token(lexer, TokenType.Keyword, db.Location(filepath, 10u32, 1u32, 1u32))
  token3 = Token(lexer, TokenType.Ident, db.Location(filepath, 11u32, 6u32, 2u32))
  token4 = Token(lexer, TokenType.Ident, db.Location(filepath, 18u32, 4u32, 2u32))
  token1.dump()
  token2.dump()
  newline.dump()
  token3.dump()
  token4.dump()
}

unittest eofTest {
  filepath = fp.Filepath.new("test_filepath", null(fp.Filepath), false)
  keytab = Keytab()
  lexer = Lexer(filepath, keytab, false)
  token = Token(lexer, TokenType.Eof, db.Location(filepath, 0u32, 0u32, 1u32))
  assert token.type == TokenType.Eof
}

unittest newValueTokenTest {
  filepath = fp.Filepath.new("test_filepath", null(fp.Filepath), false)
  keytab = Keytab()
  lexer = Lexer(filepath, keytab, false)
  location = db.Location(filepath, 0u32, 0u32, 1u32)
  println Token.newValueToken(lexer, "string token", location)
  println Token.newValueToken(lexer, true, location)
  println Token.newValueToken(lexer, 123i8, location)
  println Token.newValueToken(lexer, 123456789, location)
  println Token.newValueToken(lexer, db.Bigint(123), location)
  println Token.newValueToken(lexer, 3.14, location)
  println Token.newValueToken(lexer, Sym.new("identifier"), location)
  keytab = Keytab()
  println Token.newValueToken(lexer, Keyword(keytab, "keyword"), location)
}

unittest isTest {
  filepath = fp.Filepath.new("test_filepath", null(fp.Filepath), false)
  keytab = Keytab()
  lexer = Lexer(filepath, keytab, false)
  location = db.Location(filepath, 0u32, 0u32, 1u32)
  assert Token.newValueToken(lexer, "string token", location).isValue("string token")
  assert !Token.newValueToken(lexer, "string token", location).isValue("bad token")
  assert Token.newValueToken(lexer, true, location).isValue(true)
  assert !Token.newValueToken(lexer, true, location).isValue(false)
  assert Token.newValueToken(lexer, 123i8, location).isValue(123i8)
  assert Token.newValueToken(lexer, 123456789, location).isValue(123456789)
  assert Token.newValueToken(lexer, db.Bigint(123), location).isValue(db.Bigint(123))
  assert Token.newValueToken(lexer, 3.14, location).isValue(3.14)
  assert Token.newValueToken(lexer, Sym.new("identifier"), location).isValue(Sym.new("identifier"))
}
